{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from keras.layers.pooling import MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import and Preprocess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reading CSV from link\n",
    "def read_csv_from_link(url):\n",
    "    path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
    "    df = pd.read_csv(path,delimiter=\"\\t\",error_bad_lines=False, header=None)\n",
    "    return df\n",
    "\n",
    "# Loading All Data\n",
    "tamil_train = read_csv_from_link('https://drive.google.com/file/d/15auwrFAlq52JJ61u7eSfnhT9rZtI5sjk/view?usp=sharing')\n",
    "tamil_dev = read_csv_from_link('https://drive.google.com/file/d/1Jme-Oftjm7OgfMNLKQs1mO_cnsQmznRI/view?usp=sharing')\n",
    "tamil_test = read_csv_from_link('https://drive.google.com/file/d/10RHrqXvIKMdnvN_tVJa_FAm41zaeC8WN/view?usp=sharing')\n",
    "\n",
    "# Tamil Preprocess\n",
    "tamil_train = tamil_train.iloc[:, 0:2]\n",
    "tamil_train = tamil_train.rename(columns={0: \"text\", 1: \"label\"})\n",
    "tamil_dev = tamil_dev.iloc[:, 0:2]\n",
    "tamil_dev = tamil_dev.rename(columns={0: \"text\", 1: \"label\"})\n",
    "\n",
    "# Stats\n",
    "tamil_train['label'] = pd.Categorical(tamil_train.label)\n",
    "tamil_dev['label'] = pd.Categorical(tamil_dev.label)\n",
    "print(tamil_train['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "characters = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','t','u','v','w','x','y','z']\n",
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
    "    return text\n",
    "def preprocess(text):\n",
    "    text = emoji.demojize(text, delimiters=(\"\", \"\"))\n",
    "    #text = convert_emoticons(text)\n",
    "    res = text.lower()\n",
    "    res = res.replace('_', ' ')\n",
    "    res = res.replace('.', ' ')\n",
    "    res = res.replace(',', ' ')\n",
    "    res = res.strip()\n",
    "    words = res.split()\n",
    "    for i,word in enumerate(words):\n",
    "        if(word[0] in characters):\n",
    "            if(len(word)<3): continue\n",
    "            while words[i][-1]==words[i][-2]:\n",
    "                if(len(words[i])<2): break\n",
    "                words[i] = words[i][:-1]\n",
    "                if(len(words[i])<2): break\n",
    "    sen = \" \".join(words)\n",
    "    return sen\n",
    "    \n",
    "train_text = []\n",
    "for key, value in tamil_train['text'].iteritems(): \n",
    "  train_text.append(preprocess(value))\n",
    "\n",
    "dev_text = []\n",
    "for key, value in tamil_dev['text'].iteritems(): \n",
    "  dev_text.append(preprocess(value))\n",
    "tamil_train['text'] = pd.DataFrame(train_text)\n",
    "tamil_dev['text'] = pd.DataFrame(dev_text)\n",
    "\n",
    "corpus = []\n",
    "for i,sen in enumerate(tamil_train['text']):\n",
    "    if(tamil_train[label][i]=='not-Tamil '):\n",
    "        continue\n",
    "    if i==0: continue\n",
    "    corpus.append(preprocess(tamil_train['text'][i]))\n",
    "for i,sen in enumerate(tamil_dev['text']):\n",
    "    if(tamil_train[label][i]=='not-Tamil '):\n",
    "        continue\n",
    "    if i==0: continue\n",
    "    corpus.append(preprocess(tamil_dev['text'][i]))\n",
    "\n",
    "with open(\"corpus.txt\", \"w\") as output:\n",
    "    output.write(str(corpus))\n",
    "\n",
    "# Train unsupervised skipgram model\n",
    "unsuper_model = fasttext.train_unsupervised('/home/punyajoy/corpus.txt',\"skipgram\", dim=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build vocabulary and inverse vocabulary dictionary\n",
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the sentences.\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    vocabulary_inv = list(sorted(vocabulary_inv))\n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return [vocabulary, vocabulary_inv]\n",
    "\n",
    "# Prepare X_train by replacing text with fasttext embeddings\n",
    "def build_input_data(sentences, labels):\n",
    "    \"\"\"\n",
    "    Maps sentences and labels to vectors based on a vocabulary.\n",
    "    \"\"\"\n",
    "    x = np.array([np.array([unsuper_model.get_word_vector(word) for word in sentence]) for sentence in sentences])\n",
    "    y = np.array(labels)\n",
    "    return [x, y]\n",
    "\n",
    "# padding sentence for uniform input size\n",
    "def pad_sentences(sentences, padding_word=\"<PAD/>\"):\n",
    "    \"\"\"\n",
    "    Pads all sentences to the same length. The length is defined by the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    sequence_length = 15\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i].strip()\n",
    "        sentence = sentence.split(\" \")\n",
    "        if(len(sentence)> sequence_length):\n",
    "            sentence = sentence[0:15]\n",
    "            padded_sentences.append(sentence)\n",
    "        else:\n",
    "            num_padding = sequence_length - len(sentence)\n",
    "            new_sentence = sentence + [padding_word] * num_padding\n",
    "            padded_sentences.append(new_sentence)\n",
    "    return padded_sentences\n",
    "\n",
    "def load_data(train_text,label):\n",
    "    \"\"\"\n",
    "    Loads and preprocessed data for the dataset.\n",
    "    Returns input vectors, labels, vocabulary, and inverse vocabulary.\n",
    "    \"\"\"\n",
    "    # Load and preprocess data\n",
    "    sentences_padded = pad_sentences(train_text)\n",
    "    #vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n",
    "    x, y = build_input_data(sentences_padded, label)\n",
    "    print(type(x))\n",
    "    return [x, y]\n",
    "\n",
    "# Loading train set\n",
    "x_train, y_train = load_data(tamil_train[\"text\"],tamil_train[\"label\"])\n",
    "# Encoding labels\n",
    "x_train = np.asarray(x_train)\n",
    "coded = dict({'Not_offensive':0, 'Offensive_Targeted_Insult_Group':1,\n",
    "       'Offensive_Targeted_Insult_Individual':2,\n",
    "     'Offensive_Untargetede':3,\n",
    "       'not-Tamil' :4,\n",
    "             'Offensive_Targeted_Insult_Other':5})\n",
    "for i,j in enumerate(y_train):\n",
    "    y_train[i] = coded[j]\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 15, 300,1)\n",
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "\n",
    "# Loading dev set\n",
    "x_dev, y_dev = load_data(tamil_dev[\"text\"],tamil_dev[\"label\"])\n",
    "for i,j in enumerate(y_dev):\n",
    "    y_dev[i] = coded[j]\n",
    "    \n",
    "x_dev = x_dev.reshape(x_dev.shape[0], 15, 300, 1)\n",
    "y_dev = to_categorical(y_dev)\n",
    "\n",
    "# Loading test set\n",
    "\n",
    "x_test, y_test = load_data(tamil_test[0],tamil_test[1])\n",
    "x_test = x_test.reshape(x_test.shape[0],15,300,1)\n",
    "for i,j in enumerate(y_test):\n",
    "    y_test[i] = coded[j]\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.pooling import MaxPooling2D\n",
    "inputs = Input(shape=(15, 300, 1))\n",
    "#embedding = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=sequence_length)(inputs)\n",
    "\n",
    "conv_0 = Conv2D(64, kernel_size=5, activation='relu', padding='valid')(inputs)\n",
    "conv_1 = Conv2D(32, kernel_size=3, activation='relu', padding='valid')(conv_0)\n",
    "conv_2 = Conv2D(32, kernel_size=3, activation='relu', padding='valid')(conv_1)\n",
    "drop = Dropout(0.6)(conv_2)\n",
    "conv_3 = Conv2D(16, kernel_size=3, activation='relu')(drop)\n",
    "pool0 = MaxPooling2D(pool_size=(2, 2), padding='valid')(conv_1)\n",
    "conv_4 = Conv2D(16, kernel_size=3, activation='relu')(pool0)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2), padding='valid')(conv_2)\n",
    "# pool2 = MaxPooling2D(pool_size=(2, 2), padding='valid')(conv_3)\n",
    "# maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "# maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "# maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "# concatenated_tensor = Concatenate(axis=1)([pool0, pool1, pool2])\n",
    "flatten = Flatten()(pool1)\n",
    "hidden1 = Dense(128, activation='relu')(flatten)\n",
    "output = Dense(6, activation='softmax')(hidden1)\n",
    "\n",
    "\n",
    "# this creates a model that includes\n",
    "model1 = Sequential()       # To train the model on dataset\n",
    "model2 = Sequential()       # To extract embeddings from cnn layer\n",
    "model1 = Model(inputs=inputs, outputs=output)\n",
    "model2 = Model(inputs=inputs, outputs=hidden1)\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "\n",
    "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "epoch = 50\n",
    "cnt=0\n",
    "best_f1 = 0\n",
    "for i in range(epoch):\n",
    "    model1.fit(x_train, y_train, epochs=1,validation_data = (x_dev,y_dev))\n",
    "    pred = model1.predict(x_train)\n",
    "    prediction = []\n",
    "    for i,j in enumerate(pred):\n",
    "        a = np.argmax(j)\n",
    "        prediction.append(a)\n",
    "    y_true =[]\n",
    "    for i,j in enumerate(y_train):\n",
    "        a = np.argmax(j)\n",
    "        y_true.append(a)\n",
    "    train_f1 = f1_score(y_true, prediction, average='weighted')\n",
    "    print(\"train f1 - \",train_f1)\n",
    "    \n",
    "    \n",
    "    pred = model1.predict(x_dev)\n",
    "    prediction = []\n",
    "    for i,j in enumerate(pred):\n",
    "        a = np.argmax(j)\n",
    "        prediction.append(a)\n",
    "\n",
    "    y_true =[]\n",
    "    for i,j in enumerate(y_dev):\n",
    "        a = np.argmax(j)\n",
    "        y_true.append(a)\n",
    "    \n",
    "    val_f1 = f1_score(y_true, prediction, average='weighted')\n",
    "    # Updating best F1 socre and saving corresponding embeddings\n",
    "    if(val_f1>best_f1):\n",
    "        cnt =0\n",
    "        best_f1 = val_f1\n",
    "        x_train_dense_cnn = model2.predict(x_train)\n",
    "        x_dev_dense_cnn = model2.predict(x_dev)\n",
    "        x_test_dense_cnn = model2.predict(x_test)\n",
    "        np.save('/home/punyajoy/Dravidian_Offensive_Classification/sentence_embeddings/cnn_emb_dev_128_tamil.npy',x_dev_dense_cnn)\n",
    "        np.save('/home/punyajoy/Dravidian_Offensive_Classification/sentence_embeddings/cnn_emb_train_128_tamil.npy',x_train_dense_cnn)\n",
    "        np.save('/home/punyajoy/Dravidian_Offensive_Classification/sentence_embeddings/cnn_emb_test_128_tamil.npy',x_test_dense_cnn)\n",
    "        \n",
    "    else:\n",
    "        cnt+=1\n",
    "    # loop break condition\n",
    "    if(cnt>=7):\n",
    "        print(\"NO increase for 5 itr, breaking....\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-nlp]",
   "language": "python",
   "name": "conda-env-.conda-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
