{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading CSV from link\n",
    "def read_csv_from_link(url):\n",
    "    path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
    "    df = pd.read_csv(path,delimiter=\"\\t\",error_bad_lines=False, header=None)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading All Data\n",
    "# tamil_train = read_csv_from_link('https://drive.google.com/file/d/15auwrFAlq52JJ61u7eSfnhT9rZtI5sjk/view?usp=sharing')\n",
    "# tamil_dev = read_csv_from_link('https://drive.google.com/file/d/1Jme-Oftjm7OgfMNLKQs1mO_cnsQmznRI/view?usp=sharing')\n",
    "malayalam_train = read_csv_from_link('https://drive.google.com/file/d/13JCCr-IjZK7uhbLXeufptr_AxvsKinVl/view?usp=sharing')\n",
    "malayalam_dev = read_csv_from_link('https://drive.google.com/file/d/1J0msLpLoM6gmXkjC6DFeQ8CG_rrLvjnM/view?usp=sharing')\n",
    "# kannada_train = read_csv_from_link('https://drive.google.com/file/d/1BFYF05rx-DK9Eb5hgoIgd6EcB8zOI-zu/view?usp=sharing')\n",
    "# kannada_dev = read_csv_from_link('https://drive.google.com/file/d/1V077dMQvscqpUmcWTcFHqRa_vTy-bQ4H/view?usp=sharing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mal Preprocess\n",
    "malayalam_train = malayalam_train.iloc[:, 0:2]\n",
    "malayalam_train = malayalam_train.rename(columns={0: \"text\", 1: \"label\"})\n",
    "# Stats\n",
    "malayalam_train['label'] = pd.Categorical(malayalam_train.label)\n",
    "# Mal Preprocess\n",
    "malayalam_dev = malayalam_dev.iloc[:, 0:2]\n",
    "malayalam_dev = malayalam_dev.rename(columns={0: \"text\", 1: \"label\"})\n",
    "# Stats\n",
    "malayalam_dev['label'] = pd.Categorical(malayalam_dev.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Nos: 2\n",
      "Tesla P100-PCIE-12GB\n",
      "Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU Nos: {}\".format(torch.cuda.device_count()))\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.get_device_name(1))\n",
    "\n",
    "# Change Device - CPU/GPU-0/GPU-1\n",
    "torch.cuda.set_device(1)\n",
    "device = 'cuda'\n",
    "device = device if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter Path of Saved model here in torch.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['XLMroberta_custom_pretrained_Malayalam.pth',\n",
       " 'Distilbert_m_base_cased_Malayalam_weighted.pth',\n",
       " 'Indic_bert_Malayalam_weighted.pth',\n",
       " 'XLMroberta_base_Malayalam_weighted.pth',\n",
       " 'Distilbert_m_base_cased_Malayalam.pth',\n",
       " 'XLMroberta_custom_pretrained_Malayalam_weighted.pth',\n",
       " 'MURIL_cased_temp_Malayalam.pth',\n",
       " 'XLMroberta_large_Malayalam_weighted.pth',\n",
       " 'Mbert_base_cased_Malayalam.pth',\n",
       " 'XLMroberta_base_Malayalam.pth',\n",
       " 'Mbert_base_cased_Malayalam_weighted.pth',\n",
       " 'XLMroberta_large_Malayalam.pth',\n",
       " 'Indic_bert_Malayalam.pth',\n",
       " 'MURIL_cased_temp_Malayalam_weighted.pth']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Select\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "f = [x for x in listdir('../../finetuned_models/') if 'alayalam' in x and 'offensiv' not in x and 'collated' not in x and 'fusion' not in x]\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALL models that we are ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_filenames = [\n",
    " 'XLMroberta_custom_pretrained_Malayalam.pth',\n",
    " 'Distilbert_m_base_cased_Malayalam_weighted.pth',\n",
    " 'Indic_bert_Malayalam_weighted.pth',\n",
    " 'Distilbert_m_base_cased_Malayalam.pth',\n",
    " 'XLMroberta_custom_pretrained_Malayalam_weighted.pth',\n",
    " 'MURIL_cased_temp_Malayalam.pth',\n",
    " 'XLMroberta_large_Malayalam_weighted.pth',\n",
    " 'Mbert_base_cased_Malayalam.pth',\n",
    " 'Mbert_base_cased_Malayalam_weighted.pth',\n",
    " 'XLMroberta_large_Malayalam.pth',\n",
    " 'Indic_bert_Malayalam.pth',\n",
    " 'MURIL_cased_temp_Malayalam_weighted.pth',\n",
    "    'XLMroberta_base_Malayalam_weighted.pth',\n",
    "    'XLMroberta_base_Malayalam.pth',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pretrained_keys = [\n",
    "    'xlm-roberta-base',\n",
    "    'distilbert-base-multilingual-cased',\n",
    "    'ai4bharat/indic-bert',\n",
    "    'distilbert-base-multilingual-cased',\n",
    "    'xlm-roberta-base',\n",
    "    \"simran-kh/muril-cased-temp\",\n",
    "    'xlm-roberta-large',\n",
    "    'bert-base-multilingual-cased',\n",
    "    'bert-base-multilingual-cased',\n",
    "    'xlm-roberta-large',\n",
    "    'ai4bharat/indic-bert',\n",
    "    \"simran-kh/muril-cased-temp\",\n",
    "    'xlm-roberta-base',\n",
    "    'xlm-roberta-base',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6cb1ff5a6ca4fb5bb41025ecda570e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=125.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/punyajoy/.conda/envs/nlp/lib/python3.7/site-packages/ipykernel_launcher.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/punyajoy/.conda/envs/nlp/lib/python3.7/site-packages/ipykernel_launcher.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56410e059a9240d093344c9bb0c21e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=125.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ai4bharat/indic-bert were not used when initializing AlbertForSequenceClassification: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'sop_classifier.classifier.weight', 'sop_classifier.classifier.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff75fc10565467db7b2906ad52cdc2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=125.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8eb63bd358345e4bcc213ee1972d9a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=125.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a220292d581448fd932b1766d477f696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=125.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at simran-kh/muril-cased-temp were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at simran-kh/muril-cased-temp and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c144d32cbe43358a21e65a87041077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=125.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-670b6b60c354>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../finetuned_models/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp/lib/python3.7/site-packages/transformers/models/auto/modeling_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mMODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m             return MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING[type(config)].from_pretrained(\n\u001b[0;32m-> 1282\u001b[0;31m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1283\u001b[0m             )\n\u001b[1;32m   1284\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m~/.conda/envs/nlp/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfrom_tf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m                 \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m                 raise OSError(\n",
      "\u001b[0;32m~/.conda/envs/nlp/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    593\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_storage_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 781\u001b[0;31m         \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_should_read_directly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    782\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m             \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loading Model\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "for index in range(len(model_pretrained_keys)):\n",
    "    model_name = saved_model_filenames[index]\n",
    "    pretrained_key = model_pretrained_keys[index]\n",
    "\n",
    "    if pretrained_key == 'distilbert-base-multilingual-cased':\n",
    "        tokenizer = BertTokenizer.from_pretrained(pretrained_key)\n",
    "        model = BertForSequenceClassification.from_pretrained(pretrained_key, num_labels=5)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(pretrained_key)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(pretrained_key, num_labels=5)\n",
    "\n",
    "    state_dict = torch.load(os.path.join('../../finetuned_models/', model_name))\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "\n",
    "    label_mapping = {\n",
    "        'Not_offensive': 0, \n",
    "        'not-malayalam': 1, \n",
    "        'Offensive_Targeted_Insult_Group': 2, \n",
    "        'Offensive_Untargetede': 3, \n",
    "        'Offensive_Targeted_Insult_Individual': 4\n",
    "    }\n",
    "    \n",
    "    # Collecting Text and Labels\n",
    "    train_batch_sentences = list(malayalam_train['text'])\n",
    "    train_batch_labels =  [label_mapping[x] for x in malayalam_train['label']]\n",
    "    dev_batch_sentences = list(malayalam_dev['text'])\n",
    "    dev_batch_labels =  [label_mapping[x] for x in malayalam_dev['label']]\n",
    "\n",
    "    # Convert to Tensor\n",
    "    train_encodings = tokenizer(train_batch_sentences, padding='max_length', truncation=True, max_length=64, return_tensors=\"pt\")\n",
    "    train_labels = torch.tensor(train_batch_labels)\n",
    "    dev_encodings = tokenizer(dev_batch_sentences, padding='max_length', truncation=True, max_length=64, return_tensors=\"pt\")\n",
    "    dev_labels = torch.tensor(dev_batch_labels)\n",
    "\n",
    "    # Dataset\n",
    "    from torch.utils.data import Dataset\n",
    "\n",
    "    class Malayalam_Offensive_Dataset(Dataset):\n",
    "        def __init__(self, encodings, labels, bpe = False):\n",
    "            self.encodings = encodings\n",
    "            self.labels = labels\n",
    "            self.is_bpe_tokenized = bpe\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            if not self.is_bpe_tokenized:\n",
    "                item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "            else:\n",
    "                item = {\n",
    "                    'input_ids': torch.LongTensor(self.encodings[idx].ids),\n",
    "                    'attention_mask': torch.LongTensor(self.encodings[idx].attention_mask)\n",
    "                }\n",
    "            item['labels'] = torch.tensor(self.labels[idx])\n",
    "            return item\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "    \n",
    "    # Defining Datasets\n",
    "    train_dataset = Malayalam_Offensive_Dataset(train_encodings, train_labels, bpe = False)\n",
    "    dev_dataset = Malayalam_Offensive_Dataset(dev_encodings, dev_labels, bpe = False)\n",
    "\n",
    "    ### Run Models-1\n",
    "    from torch.utils.data import DataLoader\n",
    "    from tqdm.notebook import tqdm\n",
    "    from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Dataloaders\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    dev_preds_ = []\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for batch in tqdm(dev_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "            for logits in outputs[1].cpu().numpy():\n",
    "                dev_preds_.append(np.exp(logits)/np.sum(np.exp(logits)))\n",
    "\n",
    "    np.save('../../model_prediction_probs/preds_'+modelname+'.npy', dev_preds_)\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "label_mapping = {\n",
    "    'Not_offensive': 0, \n",
    "    'not-malayalam': 1, \n",
    "    'Offensive_Targeted_Insult_Group': 2, \n",
    "    'Offensive_Untargetede': 3, \n",
    "    'Offensive_Targeted_Insult_Individual': 4\n",
    "}\n",
    "\n",
    "# Collecting Text and Labels\n",
    "train_batch_sentences = list(malayalam_train['text'])\n",
    "train_batch_labels =  [label_mapping[x] for x in malayalam_train['label']]\n",
    "dev_batch_sentences = list(malayalam_dev['text'])\n",
    "dev_batch_labels =  [label_mapping[x] for x in malayalam_dev['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pretrained_keys = [\n",
    "    'xlm-roberta-base',\n",
    "    'distilbert-base-multilingual-cased',\n",
    "    'ai4bharat/indic-bert',\n",
    "    'distilbert-base-multilingual-cased',\n",
    "    'xlm-roberta-base',\n",
    "    \"simran-kh/muril-cased-temp\",\n",
    "    'xlm-roberta-large',\n",
    "    'bert-base-multilingual-cased',\n",
    "    'bert-base-multilingual-cased',\n",
    "    'xlm-roberta-large',\n",
    "    'ai4bharat/indic-bert',\n",
    "    \"simran-kh/muril-cased-temp\",\n",
    "    'xlm-roberta-base',\n",
    "    'xlm-roberta-base',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model_filenames = [\n",
    "    'XLMroberta_custom_pretrained_Malayalam.pth',\n",
    "    'Distilbert_m_base_cased_Malayalam_weighted.pth',\n",
    "    'Indic_bert_Malayalam_weighted.pth',\n",
    "    'Distilbert_m_base_cased_Malayalam.pth',\n",
    "    'XLMroberta_custom_pretrained_Malayalam_weighted.pth',\n",
    "    'MURIL_cased_temp_Malayalam.pth',\n",
    "    'XLMroberta_large_Malayalam_weighted.pth',\n",
    "    'Mbert_base_cased_Malayalam.pth',\n",
    "    'Mbert_base_cased_Malayalam_weighted.pth',\n",
    "    'XLMroberta_large_Malayalam.pth',\n",
    "    'Indic_bert_Malayalam.pth',\n",
    "    'MURIL_cased_temp_Malayalam_weighted.pth',\n",
    "    'XLMroberta_base_Malayalam_weighted.pth',\n",
    "    'XLMroberta_base_Malayalam.pth',\n",
    "]\n",
    "\n",
    "all_dev_preds = []\n",
    "for modelname in load_model_filenames:\n",
    "    all_dev_preds.append(np.load('../../model_prediction_probs/preds_'+modelname+'.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "print(len(load_model_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLMroberta_custom_pretrained_Malayalam.pth\n",
      "                                      precision    recall  f1-score   support\n",
      "\n",
      "                       Not_offensive       0.98      0.99      0.99      1779\n",
      "                       not-malayalam       0.89      0.93      0.91       163\n",
      "     Offensive_Targeted_Insult_Group       0.82      0.69      0.75        13\n",
      "               Offensive_Untargetede       0.74      0.70      0.72        20\n",
      "Offensive_Targeted_Insult_Individual       0.93      0.58      0.72        24\n",
      "\n",
      "                            accuracy                           0.97      1999\n",
      "                           macro avg       0.87      0.78      0.82      1999\n",
      "                        weighted avg       0.97      0.97      0.97      1999\n",
      "\n",
      "Distilbert_m_base_cased_Malayalam_weighted.pth\n",
      "                                      precision    recall  f1-score   support\n",
      "\n",
      "                       Not_offensive       0.97      0.96      0.97      1779\n",
      "                       not-malayalam       0.79      0.85      0.82       163\n",
      "     Offensive_Targeted_Insult_Group       0.44      0.62      0.52        13\n",
      "               Offensive_Untargetede       0.76      0.65      0.70        20\n",
      "Offensive_Targeted_Insult_Individual       0.37      0.54      0.44        24\n",
      "\n",
      "                            accuracy                           0.94      1999\n",
      "                           macro avg       0.67      0.72      0.69      1999\n",
      "                        weighted avg       0.94      0.94      0.94      1999\n",
      "\n",
      "Indic_bert_Malayalam_weighted.pth\n",
      "                                      precision    recall  f1-score   support\n",
      "\n",
      "                       Not_offensive       0.98      0.97      0.97      1779\n",
      "                       not-malayalam       0.82      0.87      0.84       163\n",
      "     Offensive_Targeted_Insult_Group       0.53      0.69      0.60        13\n",
      "               Offensive_Untargetede       0.57      0.65      0.60        20\n",
      "Offensive_Targeted_Insult_Individual       0.63      0.50      0.56        24\n",
      "\n",
      "                            accuracy                           0.95      1999\n",
      "                           macro avg       0.70      0.74      0.72      1999\n",
      "                        weighted avg       0.95      0.95      0.95      1999\n",
      "\n",
      "Distilbert_m_base_cased_Malayalam.pth\n",
      "                                      precision    recall  f1-score   support\n",
      "\n",
      "                       Not_offensive       0.97      0.99      0.98      1779\n",
      "                       not-malayalam       0.92      0.86      0.89       163\n",
      "     Offensive_Targeted_Insult_Group       0.83      0.38      0.53        13\n",
      "               Offensive_Untargetede       0.79      0.55      0.65        20\n",
      "Offensive_Targeted_Insult_Individual       0.45      0.42      0.43        24\n",
      "\n",
      "                            accuracy                           0.96      1999\n",
      "                           macro avg       0.79      0.64      0.69      1999\n",
      "                        weighted avg       0.96      0.96      0.96      1999\n",
      "\n",
      "XLMroberta_custom_pretrained_Malayalam_weighted.pth\n",
      "                                      precision    recall  f1-score   support\n",
      "\n",
      "                       Not_offensive       0.99      0.96      0.97      1779\n",
      "                       not-malayalam       0.80      0.96      0.87       163\n",
      "     Offensive_Targeted_Insult_Group       0.60      0.69      0.64        13\n",
      "               Offensive_Untargetede       0.47      0.75      0.58        20\n",
      "Offensive_Targeted_Insult_Individual       0.59      0.54      0.57        24\n",
      "\n",
      "                            accuracy                           0.95      1999\n",
      "                           macro avg       0.69      0.78      0.73      1999\n",
      "                        weighted avg       0.96      0.95      0.95      1999\n",
      "\n",
      "MURIL_cased_temp_Malayalam.pth\n",
      "                                      precision    recall  f1-score   support\n",
      "\n",
      "                       Not_offensive       0.97      0.98      0.98      1779\n",
      "                       not-malayalam       0.89      0.82      0.85       163\n",
      "     Offensive_Targeted_Insult_Group       0.69      0.69      0.69        13\n",
      "               Offensive_Untargetede       0.79      0.55      0.65        20\n",
      "Offensive_Targeted_Insult_Individual       0.71      0.50      0.59        24\n",
      "\n",
      "                            accuracy                           0.96      1999\n",
      "                           macro avg       0.81      0.71      0.75      1999\n",
      "                        weighted avg       0.96      0.96      0.96      1999\n",
      "\n",
      "XLMroberta_large_Malayalam_weighted.pth\n",
      "                                      precision    recall  f1-score   support\n",
      "\n",
      "                       Not_offensive       0.99      0.96      0.97      1779\n",
      "                       not-malayalam       0.82      0.96      0.88       163\n",
      "     Offensive_Targeted_Insult_Group       0.82      0.69      0.75        13\n",
      "               Offensive_Untargetede       0.38      0.75      0.51        20\n",
      "Offensive_Targeted_Insult_Individual       0.76      0.54      0.63        24\n",
      "\n",
      "                            accuracy                           0.95      1999\n",
      "                           macro avg       0.75      0.78      0.75      1999\n",
      "                        weighted avg       0.96      0.95      0.96      1999\n",
      "\n",
      "Mbert_base_cased_Malayalam.pth\n",
      "                                      precision    recall  f1-score   support\n",
      "\n",
      "                       Not_offensive       0.98      0.99      0.98      1779\n",
      "                       not-malayalam       0.91      0.90      0.90       163\n",
      "     Offensive_Targeted_Insult_Group       0.80      0.62      0.70        13\n",
      "               Offensive_Untargetede       0.81      0.65      0.72        20\n",
      "Offensive_Targeted_Insult_Individual       1.00      0.46      0.63        24\n",
      "\n",
      "                            accuracy                           0.97      1999\n",
      "                           macro avg       0.90      0.72      0.79      1999\n",
      "                        weighted avg       0.97      0.97      0.97      1999\n",
      "\n",
      "Mbert_base_cased_Malayalam_weighted.pth\n",
      "                                      precision    recall  f1-score   support\n",
      "\n",
      "                       Not_offensive       0.98      0.98      0.98      1779\n",
      "                       not-malayalam       0.87      0.91      0.89       163\n",
      "     Offensive_Targeted_Insult_Group       0.62      0.62      0.62        13\n",
      "               Offensive_Untargetede       0.62      0.65      0.63        20\n",
      "Offensive_Targeted_Insult_Individual       0.81      0.54      0.65        24\n",
      "\n",
      "                            accuracy                           0.96      1999\n",
      "                           macro avg       0.78      0.74      0.75      1999\n",
      "                        weighted avg       0.96      0.96      0.96      1999\n",
      "\n",
      "XLMroberta_large_Malayalam.pth\n",
      "                                      precision    recall  f1-score   support\n",
      "\n",
      "                       Not_offensive       0.98      0.99      0.98      1779\n",
      "                       not-malayalam       0.89      0.94      0.91       163\n",
      "     Offensive_Targeted_Insult_Group       0.82      0.69      0.75        13\n",
      "               Offensive_Untargetede       0.93      0.65      0.76        20\n",
      "Offensive_Targeted_Insult_Individual       0.92      0.50      0.65        24\n",
      "\n",
      "                            accuracy                           0.97      1999\n",
      "                           macro avg       0.91      0.75      0.81      1999\n",
      "                        weighted avg       0.97      0.97      0.97      1999\n",
      "\n",
      "Indic_bert_Malayalam.pth\n",
      "                                      precision    recall  f1-score   support\n",
      "\n",
      "                       Not_offensive       0.97      0.98      0.98      1779\n",
      "                       not-malayalam       0.87      0.87      0.87       163\n",
      "     Offensive_Targeted_Insult_Group       1.00      0.54      0.70        13\n",
      "               Offensive_Untargetede       0.52      0.55      0.54        20\n",
      "Offensive_Targeted_Insult_Individual       0.86      0.50      0.63        24\n",
      "\n",
      "                            accuracy                           0.96      1999\n",
      "                           macro avg       0.84      0.69      0.74      1999\n",
      "                        weighted avg       0.96      0.96      0.96      1999\n",
      "\n",
      "MURIL_cased_temp_Malayalam_weighted.pth\n",
      "                                      precision    recall  f1-score   support\n",
      "\n",
      "                       Not_offensive       0.98      0.96      0.97      1779\n",
      "                       not-malayalam       0.73      0.88      0.80       163\n",
      "     Offensive_Targeted_Insult_Group       0.69      0.69      0.69        13\n",
      "               Offensive_Untargetede       0.56      0.70      0.62        20\n",
      "Offensive_Targeted_Insult_Individual       0.75      0.50      0.60        24\n",
      "\n",
      "                            accuracy                           0.94      1999\n",
      "                           macro avg       0.74      0.75      0.74      1999\n",
      "                        weighted avg       0.95      0.94      0.95      1999\n",
      "\n",
      "XLMroberta_base_Malayalam_weighted.pth\n",
      "                                      precision    recall  f1-score   support\n",
      "\n",
      "                       Not_offensive       0.99      0.93      0.96      1779\n",
      "                       not-malayalam       0.68      0.94      0.79       163\n",
      "     Offensive_Targeted_Insult_Group       0.43      0.69      0.53        13\n",
      "               Offensive_Untargetede       0.33      0.70      0.44        20\n",
      "Offensive_Targeted_Insult_Individual       0.42      0.54      0.47        24\n",
      "\n",
      "                            accuracy                           0.92      1999\n",
      "                           macro avg       0.57      0.76      0.64      1999\n",
      "                        weighted avg       0.94      0.92      0.93      1999\n",
      "\n",
      "XLMroberta_base_Malayalam.pth\n",
      "                                      precision    recall  f1-score   support\n",
      "\n",
      "                       Not_offensive       0.98      0.98      0.98      1779\n",
      "                       not-malayalam       0.89      0.88      0.88       163\n",
      "     Offensive_Targeted_Insult_Group       0.90      0.69      0.78        13\n",
      "               Offensive_Untargetede       0.79      0.75      0.77        20\n",
      "Offensive_Targeted_Insult_Individual       0.68      0.54      0.60        24\n",
      "\n",
      "                            accuracy                           0.97      1999\n",
      "                           macro avg       0.85      0.77      0.80      1999\n",
      "                        weighted avg       0.97      0.97      0.97      1999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dev_preds, mn in zip(all_dev_preds, load_model_filenames):\n",
    "    final_dev_preds = np.argmax(dev_preds, axis = 1)\n",
    "    y_true = dev_batch_labels\n",
    "    y_pred = final_dev_preds\n",
    "    target_names = label_mapping.keys()\n",
    "    report = classification_report(y_true, y_pred, target_names=target_names)\n",
    "    print(mn)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaging Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dev_preds = np.array(all_dev_preds)\n",
    "\n",
    "# Initialise Weights\n",
    "w = np.ones(all_dev_preds.shape[0])\n",
    "softmax_w = np.exp(w)/np.sum(np.exp(w))\n",
    "\n",
    "weighted_all_dev_preds = np.array([sw*dpreds for sw, dpreds in zip(softmax_w, all_dev_preds)])\n",
    "weighted_dev_preds = np.sum(weighted_all_dev_preds, axis = 0)\n",
    "final_dev_preds = np.argmax(weighted_dev_preds, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = dev_batch_labels\n",
    "y_pred = final_dev_preds\n",
    "target_names = label_mapping.keys()\n",
    "report = classification_report(y_true, y_pred, target_names=target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      precision    recall  f1-score   support\n",
      "\n",
      "                       Not_offensive       0.98      0.99      0.98      1779\n",
      "                       not-malayalam       0.90      0.93      0.91       163\n",
      "     Offensive_Targeted_Insult_Group       0.82      0.69      0.75        13\n",
      "               Offensive_Untargetede       0.93      0.65      0.76        20\n",
      "Offensive_Targeted_Insult_Individual       0.92      0.50      0.65        24\n",
      "\n",
      "                            accuracy                           0.97      1999\n",
      "                           macro avg       0.91      0.75      0.81      1999\n",
      "                        weighted avg       0.97      0.97      0.97      1999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimising with GA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The best solution found:                                                                           \n",
      " [4.98740038 1.35822305 2.14807284 2.79479706 4.20657479 0.68412483\n",
      " 3.8948384  0.44254299 0.4904552  1.49206806 1.38916164 2.81783021\n",
      " 1.80244462 4.71719699]\n",
      "\n",
      " Objective function:\n",
      " 0.1496408463459632\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from geneticalgorithm import geneticalgorithm as ga\n",
    "\n",
    "def f(X):\n",
    "    softmax_w = np.exp(X)/np.sum(np.exp(X))\n",
    "    weighted_all_dev_preds = np.array([sw*dpreds for sw, dpreds in zip(softmax_w, all_dev_preds)])\n",
    "    weighted_dev_preds = np.sum(weighted_all_dev_preds, axis = 0)\n",
    "    final_dev_preds = np.argmax(weighted_dev_preds, axis = 1)\n",
    "\n",
    "    y_true = dev_batch_labels\n",
    "    y_pred = final_dev_preds\n",
    "    score = f1_score(y_true, y_pred, average='macro')\n",
    "    return 1-score\n",
    "\n",
    "varbound=np.array([[0, 5]]*all_dev_preds.shape[0])\n",
    "\n",
    "model=ga(function=f,dimension=all_dev_preds.shape[0],variable_type='real',variable_boundaries=varbound)\n",
    "\n",
    "model.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergence=model.report\n",
    "solution=model.output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'variable': array([4.98740038, 1.35822305, 2.14807284, 2.79479706, 4.20657479,\n",
       "        0.68412483, 3.8948384 , 0.44254299, 0.4904552 , 1.49206806,\n",
       "        1.38916164, 2.81783021, 1.80244462, 4.71719699]),\n",
       " 'function': 0.1496408463459632}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = solution['variable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "saved_model_filename = 'GA_v1_malayalam'\n",
    "\n",
    "with open(\"../../dev_preds/weights_\" + saved_model_filename + \".pickle\", 'rb') as handle:\n",
    "    mw = pickle.load(handle)\n",
    "    \n",
    "X = [mw[index][0] for index in mw.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.987400383529981,\n",
       " 1.358223048938526,\n",
       " 2.1480728379176064,\n",
       " 2.7947970649265406,\n",
       " 4.206574786073908,\n",
       " 0.6841248293693636,\n",
       " 3.8948383954170698,\n",
       " 0.44254299249817775,\n",
       " 0.4904552048096972,\n",
       " 1.4920680613973296,\n",
       " 1.389161641030305,\n",
       " 2.8178302110508153,\n",
       " 1.8024446186776188,\n",
       " 4.717196990349133]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_w = np.exp(X)/np.sum(np.exp(X))\n",
    "weighted_all_dev_preds = np.array([sw*dpreds for sw, dpreds in zip(softmax_w, all_dev_preds)])\n",
    "weighted_dev_preds = np.sum(weighted_all_dev_preds, axis = 0)\n",
    "final_dev_preds = np.argmax(weighted_dev_preds, axis = 1)\n",
    "\n",
    "y_true = dev_batch_labels\n",
    "y_pred = final_dev_preds\n",
    "target_names = label_mapping.keys()\n",
    "report = classification_report(y_true, y_pred, target_names=target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      precision    recall  f1-score   support\n",
      "\n",
      "                       Not_offensive       0.98      0.99      0.99      1779\n",
      "                       not-malayalam       0.92      0.94      0.93       163\n",
      "     Offensive_Targeted_Insult_Group       0.90      0.69      0.78        13\n",
      "               Offensive_Untargetede       0.94      0.75      0.83        20\n",
      "Offensive_Targeted_Insult_Individual       0.93      0.58      0.72        24\n",
      "\n",
      "                            accuracy                           0.98      1999\n",
      "                           macro avg       0.93      0.79      0.85      1999\n",
      "                        weighted avg       0.98      0.98      0.98      1999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8503591536540368"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saved_model_filename = 'GA_v1_malayalam'\n",
    "np.savetxt(\"../../dev_preds/\" + saved_model_filename + \".csv\", final_dev_preds, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../../model_prediction_probs/\"+saved_model_filename+\".npy\", weighted_dev_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {x:(y,z) for x, y, z in zip(load_model_filenames, np.array(solution['variable']), model_pretrained_keys)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../../dev_preds/weights_\" + saved_model_filename + \".pickle\", 'wb') as handle:\n",
    "    pickle.dump(a, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-nlp]",
   "language": "python",
   "name": "conda-env-.conda-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
