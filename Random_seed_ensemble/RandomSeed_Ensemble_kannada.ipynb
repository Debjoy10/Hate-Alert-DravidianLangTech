{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Reading CSV from link\n",
    "def read_csv_from_link(url):\n",
    "    path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
    "    df = pd.read_csv(path,delimiter=\"\\t\",error_bad_lines=False, header=None)\n",
    "    return df\n",
    "\n",
    "# Loading All Data\n",
    "kannada_train = read_csv_from_link('https://drive.google.com/file/d/1BFYF05rx-DK9Eb5hgoIgd6EcB8zOI-zu/view?usp=sharing')\n",
    "kannada_dev = read_csv_from_link('https://drive.google.com/file/d/1V077dMQvscqpUmcWTcFHqRa_vTy-bQ4H/view?usp=sharing')\n",
    "kannada_test = read_csv_from_link('https://drive.google.com/file/d/1Px2CvIkLP_xaNhz_fCofW-7GGBCnSYsa/view?usp=sharing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kannada Preprocess\n",
    "kannada_train = kannada_train.iloc[:, 0:2]\n",
    "kannada_train = kannada_train.rename(columns={0: \"text\", 1: \"label\"})\n",
    "kannada_dev = kannada_dev.iloc[:, 0:2]\n",
    "kannada_dev = kannada_dev.rename(columns={0: \"text\", 1: \"label\"})\n",
    "\n",
    "# Stats\n",
    "kannada_train['label'] = pd.Categorical(kannada_train.label)\n",
    "kannada_dev['label'] = pd.Categorical(kannada_dev.label)\n",
    "print(kannada_train['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Device - CPU/GPU-0/GPU-1\n",
    "torch.cuda.set_device(0)\n",
    "device = 'cuda'\n",
    "device = device if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Dataset\n",
    "class kannada_Offensive_Dataset(Dataset):\n",
    "    def __init__(self, encodings, labels, bpe = False):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.is_bpe_tokenized = bpe\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if not self.is_bpe_tokenized:\n",
    "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        else:\n",
    "            item = {\n",
    "                'input_ids': torch.LongTensor(self.encodings[idx].ids),\n",
    "                'attention_mask': torch.LongTensor(self.encodings[idx].attention_mask)\n",
    "            }\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of random seeds to try\n",
    "r_seeds = [5,10,15,23,45,52,100,150,210,500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=6)\n",
    "model_name = 'Mbert_base_cased_kannada'\n",
    "\n",
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "label_mapping = {\n",
    "    'Not_offensive': 0, \n",
    "    'not-Kannada': 1,\n",
    "    'Offensive_Targeted_Insult_Other': 2,\n",
    "    'Offensive_Targeted_Insult_Group': 3, \n",
    "    'Offensive_Untargetede': 4, \n",
    "    'Offensive_Targeted_Insult_Individual': 5\n",
    "}\n",
    "\n",
    "# Collecting Text and Labels\n",
    "train_batch_sentences = list(kannada_train['text'])\n",
    "train_batch_labels =  [label_mapping[x] for x in kannada_train['label']]\n",
    "dev_batch_sentences = list(kannada_dev['text'])\n",
    "dev_batch_labels =  [label_mapping[x] for x in kannada_dev['label']]\n",
    "\n",
    "# Defining Datasets\n",
    "train_dataset = kannada_Offensive_Dataset(train_encodings, train_labels, bpe = False)\n",
    "dev_dataset = kannada_Offensive_Dataset(dev_encodings, dev_labels, bpe = False)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "best_val_f1 = 0\n",
    "count = 0\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in r_seeds:\n",
    "    random.seed(i)\n",
    "    np.random.seed(i)\n",
    "    torch.manual_seed(i)\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "        print(\"==========================================================\")\n",
    "        print(\"Epoch {}\".format(epoch))\n",
    "        print(\"Train\")\n",
    "        for batch in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            if loss_weighted:\n",
    "                loss = loss_function(outputs[1], labels)\n",
    "            else:\n",
    "                loss = outputs[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            for logits in outputs[1].detach().cpu().numpy():\n",
    "                train_preds.append(np.argmax(logits))\n",
    "            for logits in labels.cpu().numpy():\n",
    "                train_labels.append(logits)\n",
    "            total_train_loss += loss.item()/len(train_loader)\n",
    "\n",
    "        print(\"Dev\")\n",
    "        dev_preds = []\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.set_grad_enabled(False):\n",
    "            for batch in tqdm(dev_loader):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                if loss_weighted:\n",
    "                    loss = loss_function(outputs[1], labels)\n",
    "                else:\n",
    "                    loss = outputs[0]\n",
    "                total_val_loss += loss.item()/len(dev_loader)\n",
    "\n",
    "                for logits in outputs[1].cpu().numpy():\n",
    "                    dev_preds.append(np.argmax(logits))\n",
    "\n",
    "        y_true = dev_batch_labels\n",
    "        y_pred = dev_preds\n",
    "        target_names = label_mapping.keys()\n",
    "        train_report = classification_report(train_labels, train_preds, target_names=target_names)\n",
    "        report = classification_report(y_true, y_pred, target_names=target_names)\n",
    "        val_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "        \n",
    "        # Save Best Model\n",
    "        if val_f1 > best_val_f1:\n",
    "            PATH = '../../finetuned_models/' + model_name +'_seed_' + i + '.pth'\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            model.save_pretrained(os.path.join('../../finetuned_berts/', (model_name+'_seed_' + i)))\n",
    "            best_val_f1 = val_f1\n",
    "            count = 0\n",
    "        else:\n",
    "            count += 1\n",
    "\n",
    "        print(train_report)\n",
    "        print(report)\n",
    "        print(\"Epoch {}, Train Loss = {}, Val Loss = {}, Val F1 = {}, Best Val f1 = {}, stagnant = {}\".format(epoch, total_train_loss, total_val_loss, val_f1, best_val_f1, count))\n",
    "        if count == 5:\n",
    "            print(\"No increase for 5 epochs, Stopping ...\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-nlp]",
   "language": "python",
   "name": "conda-env-.conda-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
